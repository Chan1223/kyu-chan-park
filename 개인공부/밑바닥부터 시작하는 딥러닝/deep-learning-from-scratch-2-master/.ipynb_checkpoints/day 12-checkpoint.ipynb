{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129a011c",
   "metadata": {},
   "source": [
    "# 7장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb08950",
   "metadata": {},
   "source": [
    "### 문장 생성 구현(Rnnlm 클래스 상속)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee636cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.functions import softmax\n",
    "from ch06.rnnlm import Rnnlm\n",
    "from ch06.better_rnnlm import BetterRnnlm\n",
    "\n",
    "class RnnlmGen(Rnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size = 100):\n",
    "        \"\"\" 문장 생성하는 메소드\n",
    "        \n",
    "        Args:\n",
    "            start_id: 가장 최초로 입력되는 단어 ID\n",
    "            skip_ids: 해당 리스트에 속하는 ID값의 단어는 Softmax 결과값에서 샘플링할 때 포함시키지 않을 단어들\n",
    "            sample_size: 해당 time_step으로 예측할 단어 개수\n",
    "        \n",
    "        \"\"\"\n",
    "        word_ids = [start_id]\n",
    "        \n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1) # predict가 받아들일 수 있는 미니배치 데이터로 만들기 위해 2차원으로 변환\n",
    "            \n",
    "            score = self.predict(x)       # 각 단어의 점수 출력(정규화 되기 전)\n",
    "            p = softmax(score.flatten())  # 소프트 맥스 함수로 정규화  \n",
    "            \n",
    "            sampled = np.random.choice(len(p), size=1, p=p)\n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))\n",
    "                \n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31452f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you inform founded metropolitan trap nye saab-scania vary r.h. diet poison we glamorous pretty irs expanded field asked audio n.j eurobonds downright toxic rash fray september hall principles convex garcia altered shelters seats penney sri note trouble financial backing frame namely test recruit completed burmah consequence emergency insure frame standpoint system beverage least harold ceiling ally tele-communications corn arctic rumor video authorized illusion bruce favorite denounced suburb separately many u.k. communists superior pension pit nuclear simply dangers squibb cancellation hammack beneficiaries meetings fare somewhat cancel experience response withdrawals lambert rebel backdrop accounted somewhat toxic consortium february threatening reinvest committed alliances\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *\n",
    "# from rnnlm_gen import RnnlmGen\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "model = RnnlmGen()\n",
    "# model.load_params('../ch06/Rnnlm.pkl')\n",
    "\n",
    "# 시작(start) 문자와 건너뜀(skip) 문자 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "# PTB 데이터셋은 원래 문장에 전처리를 해놨음\n",
    "# <unk>는 unique, N은 숫자, \n",
    "skip_words = ['N', '<unk>', '$']  \n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee00d249",
   "metadata": {},
   "source": [
    "### 앞 모델에서 학습이 끝낸 가중치를 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d3c719ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you know he told never.\n",
      " mr. jones says the kemper staff will market serious a whole view mr. lawson says that it will become program.\n",
      " he 's strong sweet elections stanza a long-term product of those forces a big major equity issue are likely to be in a regulated line.\n",
      " on preferred inflation followed by a joint executive vice president of this maker and u.s. affiliate.\n",
      " mr. reitman said the director of the airline 's hostile marketer.\n",
      " he is out of new davis seeking late august of nov..\n",
      " mr. freeman was elected the furukawa\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *\n",
    "# from rnnlm_gen import RnnlmGen\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params('ch06/Rnnlm.pkl') # 이미 학습된 가중치를 사용\n",
    "\n",
    "# 시작(start) 문자와 건너뜀(skip) 문자 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff04d279",
   "metadata": {},
   "source": [
    "### Toy problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe9136ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 7) (45000, 5)\n",
      "(5000, 7) (5000, 5)\n",
      "[ 3  0  2  0  0 11  5]\n",
      "[ 6  0 11  7  5]\n",
      "71+118 \n",
      "_189 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import sequence\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt', seed=1984)\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "print(x_train.shape, t_train.shape)\n",
    "print(x_test.shape, t_test.shape)\n",
    "\n",
    "print(x_train[0])\n",
    "print(t_train[0])\n",
    "\n",
    "print(''.join([id_to_char[c] for c in x_train[0]]))\n",
    "print(''.join([id_to_char[c] for c in t_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e1603",
   "metadata": {},
   "source": [
    "### Encoder class 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b1b28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, statful = False)\n",
    "        \n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97783837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 클래스 구현하기 -> LSTM 모델 사용\n",
    "import numpy as np\n",
    "from common.time_layers import TimeEmbedding, TimeLSTM, TimeAffine, TimeSoftmaxWithLoss\n",
    "\n",
    "class Encoder:\n",
    "    \"\"\" LSTM 모델 기반으로 하는 Encoder 클래스\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: 주어진 말뭉치 내 unique한 단어 개수(Vocabulary size)\n",
    "        wordvec_size: One-hot으로 되어있는 Sparse 입력 단어를 몇 차원 임베딩 Dense 벡터로 줄일 것인지\n",
    "        hidden_size: LSTM 계층 내의 은닉 상태 벡터 차원 수(노드 수)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4*H).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False) # Encoder는 중간에 출력되는 은닉상태 벡터 생략\n",
    "        \n",
    "        # 모든 계층의 파라미터 취합\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = [self.embed, self.lstm]\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        self.hs = None  # Encoder에서 내뱉을 T길이의 은닉상태 벡터값들. 그런데 이 중 마지막 값만 필요함!\n",
    "        \n",
    "    \n",
    "    def forward(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        hs = xs\n",
    "        self.hs = hs\n",
    "        return hs[:, -1, :]  # 마지막 은닉상태 벡터만 추출\n",
    "    \n",
    "    \n",
    "    def backward(self, dh):\n",
    "        dout = np.zeros_like(self.hs)   # T길이의 은닉상태 벡터 값들 빈 껍데기 형상 만들어놓기 for dh 넣을 위치 마련\n",
    "        dout[:, -1, :] = dh             # 이 때, dh는 Decoder에서 역전파로 흘러들어오는 기울기 값\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1f12e",
   "metadata": {},
   "source": [
    "### Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cea093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 클래스 구현하기 -> LSTM 모델 사용\n",
    "class Decoder:\n",
    "    \"\"\" LSTM 모델 기반으로 하는 Decoder 클래스(단, Softmax-with-Loss 계층은 포함 X)\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: 주어진 말뭉치 내 unique한 단어 개수(Vocabulary size)\n",
    "        wordvec_size: One-hot으로 되어있는 Sparse 입력 단어를 몇 차원 임베딩 Dense 벡터로 줄일 것인지\n",
    "        hidden_size: LSTM 계층 내의 은닉 상태 벡터 차원 수(노드 수)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        # Embedding -> LSTM -> Affine 계층\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4*H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        \n",
    "        self.layers = [self.embed, self.lstm, self.affine]\n",
    "        \n",
    "        # 모든 계층 파라미터 취합\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "        \n",
    "    def forward(self, xs, h):\n",
    "        \"\"\" Decoder 학습 시 순전파 수행\n",
    "        \n",
    "        Args:\n",
    "            xs: T길이 시계열 데이터(학습이기 떄문에 이미 정답을 알고 있음!)\n",
    "            h: Encoder에서 전달해준 은닉 상태 벡터\n",
    "        \n",
    "        \"\"\"\n",
    "        self.lstm.set_state(h)  # Decoder의 최초 은닉 상태 입력으로서 Encoder에서 전달해준 h 설정!\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        score = xs\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def backward(self, dscore):\n",
    "        \"\"\" Decoder 역전파 수행\n",
    "        \n",
    "        Args:\n",
    "            dscore: Softmax 계층으로부터 흘러들어온 국소적인 미분값\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            dscore = layer.backward(dscore)\n",
    "            \n",
    "        # Encoder로 역전파를 수행할 때 전달해줄 h의 기울기 값!\n",
    "        dh = self.lstm.dh\n",
    "        return dh\n",
    "    \n",
    "    \n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        \"\"\" Decoder에서 테스트 시 문장 생성하는 메소드\n",
    "        \n",
    "        Args:\n",
    "            h: Encoder에서 전달해준 은닉 상태(Decoder에서 최초로 입력되는 은닉상태)\n",
    "            start_id: h와 같이 Decoder에서 최초로 입력되는 데이터. 보통 <eos>와 같은 기호임\n",
    "            sample_size: 생성하는 문자 개수\n",
    "        \n",
    "        \"\"\"\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        self.lstm.set_state(h)  # Encoder가 전달해준 은닉 상태 Decoder에서 최초의 은닉상태로 입력 설정!\n",
    "        \n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape((1, 1))  # Mini-batch로 구현되므로 2d-array롤 변경\n",
    "            for layer in self.layers:\n",
    "                x = layer.forward(x)\n",
    "            score = x\n",
    "            \n",
    "            # score이 가장 높은 index(문자ID) 추출\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "        \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b21185",
   "metadata": {},
   "source": [
    "### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a02d5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-Decoder 연결 및 TimeSoftmax-with-Loss 계층 추가\n",
    "class Seq2Seq:\n",
    "    \"\"\" Encoder-Decoder 연결 및 TimeSoftmax-with-Loss 계층 추가한 전체 Seq2Seq 모델 클래스\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: 주어진 말뭉치 내 unique한 단어 개수(Vocabulary size)\n",
    "        wordvec_size: One-hot으로 되어있는 Sparse 입력 단어를 몇 차원 임베딩 Dense 벡터로 줄일 것인지\n",
    "        hidden_size: LSTM 계층 내의 은닉 상태 벡터 차원 수(노드 수)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "        \n",
    "        \n",
    "    def forward(self, xs, ts):\n",
    "        # 정답 레이블로부터 Decoder의 입력 / 출력(정답) 데이터로 분할 -> 학습 시이기 때문!\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "        \n",
    "        h = self.encoder.forward(xs)                         # Encoder\n",
    "        score = self.decoder.forward(decoder_xs, h)          # Decoder\n",
    "        loss = self.loss_layer.forward(score, decoder_ts)    # Softmax-with-Loss\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dscore = self.loss_layer.backward(dout)\n",
    "        dh = self.decoder.backward(dscore)\n",
    "        dx = self.encoder.backward(dh)\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        # 테스트 시, Decoder에서 문장 생성!\n",
    "        h = self.encoder.forward(xs)     # Encoder에서 은닉상태 반환\n",
    "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac3185",
   "metadata": {},
   "source": [
    "###  Peeky Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "53cb5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peeky Decoder 클래스 구현\n",
    "import numpy as np\n",
    "from common.time_layers import *\n",
    "from ch07.seq2seq import * \n",
    "\n",
    "\n",
    "class PeekyDecoder:\n",
    "    \"\"\" 기존 Deecoder 부분을 Peeky Decoder로 변경한 클래스\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: 주어진 말뭉치 내 unique한 단어 개수(Vocabulary size)\n",
    "        wordvec_size: One-hot으로 되어있는 Sparse 입력 단어를 몇 차원 임베딩 Dense 벡터로 줄일 것인지\n",
    "        hidden_size: LSTM 계층 내의 은닉 상태 벡터 차원 수(노드 수)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        # Embedding -> LSTM -> Affine\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(H + D, 4*H) / np.sqrt(H + D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4*H).astype('f')\n",
    "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)  # Decoder이기 때문에 은닉 상태 유지!\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        \n",
    "        # 모든 계층 파라미터 취합\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        self.cache = None\n",
    "        \n",
    "        \n",
    "    def forward(self, xs, h):\n",
    "        \"\"\" Peeky Decoder 순전파 수행(학습 시))\n",
    "        \n",
    "        Args:\n",
    "            xs: (batch_size, 전체 시계열 길이(T)) Decoder가 내뱉는 도착어. 학습 시이기 때문에 정답 시퀀스나 마찬가지\n",
    "            h: (batch_size, 은닉상태 벡터 차원 수) Encoder가 전달하는 최종 은닉상태 벡터\n",
    "        \"\"\"\n",
    "        N, T = xs.shape\n",
    "        N, H = h.shape\n",
    "        \n",
    "        self.lstm.set_state(h) # Encoder가 전달하는 은닉상태 벡터 입력!\n",
    "        \n",
    "        # Embedding 순전파 후 concatenate\n",
    "        out = self.embed.forward(xs)\n",
    "        # np.repeat(array, cnt): array를 cnt번 복제\n",
    "        hs = np.repeat(h, T, axis=0).reshape(N, T, H) # Encoder가 전달한 은닉상태(h)를 T길이 만큼 복제(다른 계층에도 전달하기 위해)\n",
    "        out = np.concatenate((hs, out), axis=2)  # 마지막 차원(axis=0,axis=1,axis=2) 방향으로 concat\n",
    "        \n",
    "        # LSTM 순전파 후 concatenate\n",
    "        out = self.lstm.forward(out)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "        \n",
    "        # Affine 계층 순전파\n",
    "        score = self.affine.forward(out)\n",
    "        self.cache = H\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def backward(self, dscore):\n",
    "        H = self.cache\n",
    "        \n",
    "        # Affine -> concatenate 역전파 수행\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dhs0, dout = dout[:, :, :H], dout[:, :, H:]  # dhs0: Encoder에서 전달받은 h에 대한 기울기, dout: LSTM 계층으로 흘려보낼 기울기\n",
    "        # LSTM -> concatenate 역전파 수행\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dhs1, dembed = dout[:, :, :H], dout[:, :, H:]\n",
    "        self.embed.backward(dembed)\n",
    "        \n",
    "        # 분기된 Encoder h값들에 대한 기울기 역전파하니까 sum!\n",
    "        dhs = dhs0 + dhs1\n",
    "        dh = self.lstm.dh + np.sum(dhs, axis=1)  # self.lstm.dh: TimeLSTM 계층에서 역전파 시의 마지막 dh가 캐싱되어 있도로 구현했었음!\n",
    "        \n",
    "        return dh\n",
    "    \n",
    "    \n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        char_id = start_id\n",
    "        \n",
    "        self.lstm.set_state(h)         # Encoder에서 전달한 은닉상태 입력!\n",
    "        \n",
    "        H = h.shape[1]                 # 은닉상태 벡터 차원 수\n",
    "        peeky_h = h.reshape(1, 1, H)  # 이것으로 문장 생성(Test) 시, 다른 LSTM, Affine 계층들이 Encoder가 전달하는 정보 엿봄!\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([char_id]).reshape((1, 1))\n",
    "            \n",
    "            # Embediing & concatenate\n",
    "            out = self.embed.forward(x)\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            # LSTM & concatenate\n",
    "            out = self.lstm.forward(out)\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            # Affine\n",
    "            score = self.affine.forward(out)\n",
    "            \n",
    "            # argmax\n",
    "            char_id = np.argmax(score.flatten())\n",
    "            sampled.append(char_id)\n",
    "        \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b7487",
   "metadata": {},
   "source": [
    "### PeekySeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d422923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peeky Decoder 기반으로 하는 PeekySeq2Seq 클래스 구현\n",
    "class PeekySeq2Seq(Seq2Seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        # Encoder - Decoder - Softmax 계층 설정\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = PeekyDecoder(V, D, H)\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        # 모든 계층 파라미터 취합\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.encoder, self.decoder):  # loss 계층에는 파라미터 X\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
