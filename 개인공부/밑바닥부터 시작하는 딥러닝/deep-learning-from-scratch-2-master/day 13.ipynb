{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6164b18",
   "metadata": {},
   "source": [
    "# 8장 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af342f26",
   "metadata": {},
   "source": [
    "### 맥락 벡터 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43f87170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4  # 시계열(T)길이, 은닉 상태 벡터(H) 설정\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1) # 앞서 만든 a를 변환\n",
    "print(ar.shape) # (5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd75afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "t = hs * ar\n",
    "print(t.shape) # (5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bf49bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "c = np.sum(t, axis=0) # (5, 4)에서 5를 사라지게 만듦\n",
    "print(c.shape)        # 결과적으로 (4,)만 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff8e86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8 , 0.1 , 0.03, 0.05, 0.02])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a # array([0.8 , 0.1 , 0.03, 0.05, 0.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45f8c032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "[[0.8  0.8  0.8  0.8 ]\n",
      " [0.1  0.1  0.1  0.1 ]\n",
      " [0.03 0.03 0.03 0.03]\n",
      " [0.05 0.05 0.05 0.05]\n",
      " [0.02 0.02 0.02 0.02]]\n"
     ]
    }
   ],
   "source": [
    "print(ar.shape) # (5, 4)\n",
    "\n",
    "print(ar)   # [[0.8  0.8  0.8  0.8 ]\n",
    "            #  [0.1  0.1  0.1  0.1 ]\n",
    "            #  [0.03 0.03 0.03 0.03]\n",
    "            #  [0.05 0.05 0.05 0.05]\n",
    "            #  [0.02 0.02 0.02 0.02]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad91ded",
   "metadata": {},
   "source": [
    "### 미니배치 처리용 가중합 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ff7f2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "# 2차원의 a를 3차원으로 reshape, 이후 repeat를 통해 벡터수만큼 늘림\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "# ar = a.reshape(N, T, 1), # 브로드캐스트를 사용하는 경우\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)          # (10, 5, 4)\n",
    "\n",
    "c = np.sum(t, axis = 1) # axis로 지정한 해당 축이 없어짐\n",
    "print(c.shape)          # (10, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b5abe",
   "metadata": {},
   "source": [
    "### (1) 선택 작업 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2373aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum:\n",
    "    \"\"\" Encoder의 모든 은닉 상태 hs 와 LSTM 계층에서 흘러나온 가중치 a 간의 Weighted Sum\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], [] # 학습하는 매개변수가 없기 때문에 공란으로 설정\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        \"\"\" Weighted Sum 순전파 수행\n",
    "        \n",
    "        Args:\n",
    "            hs: Encoder의 모든 은닉 상태 hs\n",
    "            a: RNN 계층에서 출력한 은닉상태(현재 shape: (batch_size, 은닉상태 차원 수)\n",
    "        \n",
    "        \"\"\"\n",
    "        N, T, H = hs.shape   # (bacth_size, 입력시퀀스 길이, 은닉상태 차원 수)\n",
    "        \n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        \"\"\" Weighted Sum 역전파 수행\n",
    "        \n",
    "        Args:\n",
    "            dc: 순전파 시, Affine 계층으로 전달한 맥락 벡터 c의 기울기 값\n",
    "        \n",
    "        \"\"\"\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)  # sum의 역전파\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        \n",
    "        da = np.sum(dar, axis=2)  # repeat의 역전파\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd385bf",
   "metadata": {},
   "source": [
    "### (2) 가중치(a) 계산 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7faba87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) 가중치(a) 계산 계층\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = softmax()\n",
    "        self.cache = None\n",
    "        \n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        \"\"\" 가중치 계산 계층에서의 순전파 수행\n",
    "        \n",
    "        Args:\n",
    "            hs: Encoder의 모든 은닉 상태 hs\n",
    "            h: RNN 계층에서 출력한 은닉상태(현재 shape: (batch_size, 은닉상태 차원 수)\n",
    "        \"\"\"\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1)  # RNN 계층에서 나온 은닉상태 repeat\n",
    "        \n",
    "        # 내적 수행하여 Encoder의 각 은닉 상태와 RNN 계층에서 나온 은닉상태 간의 유사도 각각 계산\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        \n",
    "        a = self.softmax.forward(s)\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        \"\"\" 가중치 계산 계층에서의 역전파 수행\n",
    "        \n",
    "        Args:\n",
    "            da: (1) 선택 작업 계층으로 보낸 가중치(a)의 기울기\n",
    "        \n",
    "        \"\"\"\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)   \n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)   # sum의 역전파\n",
    "        dhr = dt * hs\n",
    "        dhs = dt * hr\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4dcdf4",
   "metadata": {},
   "source": [
    "### (1), (2) 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da608c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) 결합계층\n",
    "class Attention:\n",
    "    \"\"\" (1) 은닉상태, 가중치 간 Weighted sum 계층, (2) 가중치 계산 계층을 결합하는 클래스\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()  # (2) 가중치 계산 계층\n",
    "        self.weight_sum_layer = WeightedSum()             # (1) 은닉상태, 가중치 간 Weigted sum 계층\n",
    "        self.attention_weight = None\n",
    "    \n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        \"\"\" (3) 결합 계층의 순전파\n",
    "        \n",
    "        Args:\n",
    "            hs: Encoder의 모든 은닉 상태 hs\n",
    "            h: RNN 계층에서 출력한 은닉상태(현재 shape: (batch_size, 은닉상태 차원 수)\n",
    "        \n",
    "        \"\"\"\n",
    "        # (2) 가중치 계산\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        # (1) Weighted sum 계층\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"(3) 결합 계층의 역전파\n",
    "        \n",
    "        Args:\n",
    "            dout: Affine 계층으로부터 흘러들어오고 있는 국소적인 미분값\n",
    "        \n",
    "        \"\"\"\n",
    "        # 순전파 시, hs가 분기(repeat)되어 (1),(2) 계층으로 흘러들어갔으므로 역전파 시 sum!\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe391ee",
   "metadata": {},
   "source": [
    "### Time Attention 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74222237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    \"\"\"입력 시퀀스 길이(T) 전체를 처리하는 Time Attention 계층\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        \"\"\" Time Attention 계층의 순전파(학습 시)\n",
    "        \n",
    "        Args:\n",
    "            hs_enc: Encoder의 모든 은닉 상태 hs\n",
    "            hs_dec: 출력 시퀀스 길이 만큼의 RNN 계층에서 나오는 은닉상태 벡터 값 (batch_size, 출력시퀀스 길이, 은닉상태 차원 수)\n",
    "        \n",
    "        \"\"\"\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)   # T개의 Attention 계층에서 나오는 출력값 담을 빈 껍데기 생성\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\" Time Attention 계층의 역전파(학습 시)\n",
    "        \n",
    "        Args:\n",
    "            dout: Affine 계층으로부터 흘러들어오는 기울기 값\n",
    "        \n",
    "        \"\"\"\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0                    # Encoder의 hs가 T개의 Attention 계층들로 분기되었기 때문에 이를 역전파하면 sum 하므로 이를 위한 값 초기화\n",
    "        dhs_dec = np.empty_like(dout)  # Decoder의 LSTM 계층 방향으로 역전파될 때 저장할 기울기\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "            \n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cf242",
   "metadata": {},
   "source": [
    "### 어텐션을 갖춘 seq2seq 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252c6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
